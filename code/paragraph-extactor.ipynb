{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoC to extract paragraphs from lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip stuff needed\n",
    "#%pip install openpyxl\n",
    "#%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aleksander.jakobsen\\AppData\\Roaming\\nltk_data\n",
      "[nltk_data]     ...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'filename', 'page', 'content', 'possible_language', 'langdetect',\n",
       "       'content_could_be_natural_language', 'content_scrubbed_light',\n",
       "       'content_scrubbed_light_could_be_natural_language', 'WELLBORE NAME',\n",
       "       'WELL NAME', 'TYPE', 'INFO ITEM GROUP TYPE', 'INFO ITEM TYPE', 'TITLE',\n",
       "       'DESCRIPTION', 'CREATOR BA NAME', 'Short Dataset Id',\n",
       "       'Required Dataset', 'ROW CREATED DATE', 'PUBLIC', 'PUBLIC ACCESS',\n",
       "       'RELEASE DATE', 'FILE FORMAT', 'Size', 'DATA ORGANIZATION',\n",
       "       'DATA COLLECTION', 'CREATION PROCESS', 'DATA DOMAIN', 'REMARK',\n",
       "       'UNLOAD FILE PATH', 'UNLOAD FILE NAME', 'INFORMATION ITEM ID',\n",
       "       'Dataset Id', 'Last Modified', 'Update Time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file = \"../data/Norway - Diskos reports_5.xlsx\"\n",
    "df = pd.read_excel(path_to_file)\n",
    "columns = df.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_id                                                  object\n",
       "filename                                             object\n",
       "page                                                  int64\n",
       "content                                              object\n",
       "possible_language                                    object\n",
       "langdetect                                           object\n",
       "content_could_be_natural_language                      bool\n",
       "content_scrubbed_light                               object\n",
       "content_scrubbed_light_could_be_natural_language       bool\n",
       "WELLBORE NAME                                        object\n",
       "WELL NAME                                            object\n",
       "TYPE                                                 object\n",
       "INFO ITEM GROUP TYPE                                 object\n",
       "INFO ITEM TYPE                                       object\n",
       "TITLE                                                object\n",
       "DESCRIPTION                                          object\n",
       "CREATOR BA NAME                                      object\n",
       "Short Dataset Id                                    float64\n",
       "Required Dataset                                     object\n",
       "ROW CREATED DATE                                     object\n",
       "PUBLIC                                               object\n",
       "PUBLIC ACCESS                                       float64\n",
       "RELEASE DATE                                         object\n",
       "FILE FORMAT                                          object\n",
       "Size                                                float64\n",
       "DATA ORGANIZATION                                    object\n",
       "DATA COLLECTION                                      object\n",
       "CREATION PROCESS                                     object\n",
       "DATA DOMAIN                                          object\n",
       "REMARK                                               object\n",
       "UNLOAD FILE PATH                                     object\n",
       "UNLOAD FILE NAME                                     object\n",
       "INFORMATION ITEM ID                                 float64\n",
       "Dataset Id                                           object\n",
       "Last Modified                                        object\n",
       "Update Time                                          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters:\n",
    "- \"INFO ITEM TYPE\" on \"DISCOVERY_EVALUATION_REPORT\"\n",
    "- \"content_could_be_natural_language\" = True\n",
    "- \"possible_language\" = \"en\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatonate_docs(df):\n",
    "    \"\"\"\n",
    "    Concatenates lines with same _id\n",
    "    params: df (DataFrame) - The DataFrame to process\n",
    "    returns: DataFrame with concatenated content by '_id'\n",
    "    \"\"\"\n",
    "    grouped_content = df.groupby('_id')['content'].apply(lambda x: ''.join(x.astype(str)))\n",
    "    result_df = grouped_content.reset_index(name='concatenated_content')\n",
    "    return result_df\n",
    "\n",
    "def define_passages(df, global_passage_id, passage_length):\n",
    "    \"\"\"\n",
    "    Splits a document into passages of sentences.\n",
    "    \n",
    "    Expects df on format \"_id\", \"concatenated_content\"\n",
    "    \n",
    "    Params:\n",
    "    - df (DataFrame): DataFrame with the documents concatenated by '_id'\n",
    "    - global_passage_id (int): Starting passage ID for the entire data processing\n",
    "    - passage_length (int): The number of sentences in each passage\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the columns \"_id\", \"concatenated_content\", \"Passage_id\", \"passage\"\n",
    "    - New global_passage_id after processing the given DataFrame\n",
    "    \"\"\"\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    passages_data = []\n",
    "\n",
    "    # Iterate through each document\n",
    "    for index, row in df.iterrows():\n",
    "        doc_id = row['_id']\n",
    "        content = row['concatenated_content']\n",
    "        sentences = tokenizer.tokenize(content)\n",
    "        \n",
    "        for i in range(0, len(sentences), passage_length):\n",
    "            passage = ' '.join(sentences[i:i+passage_length])\n",
    "            passages_data.append([doc_id, global_passage_id, passage])\n",
    "            global_passage_id += 1\n",
    "\n",
    "    passages_df = pd.DataFrame(passages_data, columns=['_id', 'Passage_id', 'passage'])\n",
    "    return passages_df, global_passage_id\n",
    "\n",
    "\n",
    "def base_df(df, passage_length):\n",
    "    \"\"\"\n",
    "    Process the DataFrame to return passages for all unique \"INFO ITEM TYPE\"\n",
    "\n",
    "    Params:\n",
    "    - df (DataFrame): Original DataFrame with different \"INFO ITEM TYPE\"s\n",
    "    - passage_length (int): Number of sentences that make up a passage\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with \"INFO_ITEM_TYPE\", \"_id\", \"passage_id\", \"passage\"\n",
    "    \"\"\"\n",
    "    global_passage_id = 0\n",
    "    result_dfs = []  # List to hold intermediate DataFrames\n",
    "\n",
    "    # Get unique 'INFO ITEM TYPE'\n",
    "    info_item_types = df['INFO ITEM TYPE'].unique()\n",
    "\n",
    "    # Process each unique 'INFO ITEM TYPE'\n",
    "    for item_type in info_item_types:\n",
    "        # Subset the dataframe by 'INFO ITEM TYPE'\n",
    "        df_filtered = df[(df['INFO ITEM TYPE'] == item_type) &\n",
    "                         (df['content_could_be_natural_language'] == True) &\n",
    "                         (df['possible_language'] == 'en')]\\\n",
    "                         .reset_index(drop=True)\n",
    "        \n",
    "        concat_df = concatonate_docs(df_filtered)\n",
    "        passage_df, global_passage_id = define_passages(concat_df, global_passage_id, passage_length)\n",
    "        \n",
    "        # Add back the 'INFO ITEM TYPE' to be part of the result DataFrame\n",
    "        passage_df['INFO_ITEM_TYPE'] = item_type\n",
    "        result_dfs.append(passage_df)\n",
    "\n",
    "    # Concatenate all intermediate DataFrames\n",
    "    final_df = pd.concat(result_dfs).reset_index(drop=True)\n",
    "    \n",
    "    # Reorder columns to match the desired format\n",
    "    final_df = final_df[['INFO_ITEM_TYPE', '_id', 'Passage_id', 'passage']]\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_df = base_df(df,passage_length=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INFO_ITEM_TYPE</th>\n",
       "      <th>_id</th>\n",
       "      <th>Passage_id</th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130481</th>\n",
       "      <td>DISCOVERY_EVALUATION_REPORT</td>\n",
       "      <td>59d3b7de417e13085ff1dbee2ce826ff2fe23165</td>\n",
       "      <td>130481</td>\n",
       "      <td>V FORMATION TESTER S.P.E. PL N5 Ended O.O.H.F....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130482</th>\n",
       "      <td>DISCOVERY_EVALUATION_REPORT</td>\n",
       "      <td>59d3b7de417e13085ff1dbee2ce826ff2fe23165</td>\n",
       "      <td>130482</td>\n",
       "      <td>14 mn Sampling chamber pressure reading at sur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130483</th>\n",
       "      <td>DISCOVERY_EVALUATION_REPORT</td>\n",
       "      <td>59d3b7de417e13085ff1dbee2ce826ff2fe23165</td>\n",
       "      <td>130483</td>\n",
       "      <td>trip ? Reason for testing: Oil sample for PVT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130484</th>\n",
       "      <td>DISCOVERY_EVALUATION_REPORT</td>\n",
       "      <td>601b0da72cc6a2ec5015e66636cd167a4d3802fb</td>\n",
       "      <td>130484</td>\n",
       "      <td>LIST OF CONTENT Abstract I. Introduction 2. Ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130485</th>\n",
       "      <td>DISCOVERY_EVALUATION_REPORT</td>\n",
       "      <td>601b0da72cc6a2ec5015e66636cd167a4d3802fb</td>\n",
       "      <td>130485</td>\n",
       "      <td>Appendix C: Core listing + core log. Appendix ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130703</th>\n",
       "      <td>DISCOVERY_EVALUATION_REPORT</td>\n",
       "      <td>fc177ae8d5cf8ff470803445387f91e294ee0df7</td>\n",
       "      <td>130703</td>\n",
       "      <td>Analysts Name G. W. -COOPERasp lea alio Immo a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130704</th>\n",
       "      <td>DISCOVERY_EVALUATION_REPORT</td>\n",
       "      <td>fc177ae8d5cf8ff470803445387f91e294ee0df7</td>\n",
       "      <td>130704</td>\n",
       "      <td>- '  -  . -  . . - . Average Range Uncorr for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130705</th>\n",
       "      <td>DISCOVERY_EVALUATION_REPORT</td>\n",
       "      <td>fc177ae8d5cf8ff470803445387f91e294ee0df7</td>\n",
       "      <td>130705</td>\n",
       "      <td>-... - . . . iIMO II= OM NM MI 11111 MI Mill M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130706</th>\n",
       "      <td>DISCOVERY_EVALUATION_REPORT</td>\n",
       "      <td>fc177ae8d5cf8ff470803445387f91e294ee0df7</td>\n",
       "      <td>130706</td>\n",
       "      <td>2C 4814m 28.6.76 08.00-12.30 - - 10 litres mud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130707</th>\n",
       "      <td>DISCOVERY_EVALUATION_REPORT</td>\n",
       "      <td>fc177ae8d5cf8ff470803445387f91e294ee0df7</td>\n",
       "      <td>130707</td>\n",
       "      <td>FITs 1,1A,1B,2,2A,2B Rain with open port, shap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     INFO_ITEM_TYPE                                       _id   \n",
       "130481  DISCOVERY_EVALUATION_REPORT  59d3b7de417e13085ff1dbee2ce826ff2fe23165  \\\n",
       "130482  DISCOVERY_EVALUATION_REPORT  59d3b7de417e13085ff1dbee2ce826ff2fe23165   \n",
       "130483  DISCOVERY_EVALUATION_REPORT  59d3b7de417e13085ff1dbee2ce826ff2fe23165   \n",
       "130484  DISCOVERY_EVALUATION_REPORT  601b0da72cc6a2ec5015e66636cd167a4d3802fb   \n",
       "130485  DISCOVERY_EVALUATION_REPORT  601b0da72cc6a2ec5015e66636cd167a4d3802fb   \n",
       "...                             ...                                       ...   \n",
       "130703  DISCOVERY_EVALUATION_REPORT  fc177ae8d5cf8ff470803445387f91e294ee0df7   \n",
       "130704  DISCOVERY_EVALUATION_REPORT  fc177ae8d5cf8ff470803445387f91e294ee0df7   \n",
       "130705  DISCOVERY_EVALUATION_REPORT  fc177ae8d5cf8ff470803445387f91e294ee0df7   \n",
       "130706  DISCOVERY_EVALUATION_REPORT  fc177ae8d5cf8ff470803445387f91e294ee0df7   \n",
       "130707  DISCOVERY_EVALUATION_REPORT  fc177ae8d5cf8ff470803445387f91e294ee0df7   \n",
       "\n",
       "       Passage_id                                            passage  \n",
       "130481     130481  V FORMATION TESTER S.P.E. PL N5 Ended O.O.H.F....  \n",
       "130482     130482  14 mn Sampling chamber pressure reading at sur...  \n",
       "130483     130483  trip ? Reason for testing: Oil sample for PVT ...  \n",
       "130484     130484  LIST OF CONTENT Abstract I. Introduction 2. Ar...  \n",
       "130485     130485  Appendix C: Core listing + core log. Appendix ...  \n",
       "...           ...                                                ...  \n",
       "130703     130703  Analysts Name G. W. -COOPERasp lea alio Immo a...  \n",
       "130704     130704  - '  -  . -  . . - . Average Range Uncorr for ...  \n",
       "130705     130705  -... - . . . iIMO II= OM NM MI 11111 MI Mill M...  \n",
       "130706     130706  2C 4814m 28.6.76 08.00-12.30 - - 10 litres mud...  \n",
       "130707     130707  FITs 1,1A,1B,2,2A,2B Rain with open port, shap...  \n",
       "\n",
       "[227 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `df` is the DataFrame obtained from the base_df function\n",
    "disc_eval_df  = desired_df[desired_df['INFO_ITEM_TYPE'] == 'DISCOVERY_EVALUATION_REPORT']\n",
    "disc_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def concatonate_docs(df):\n",
    "#     \"\"\"\n",
    "#     Concatonates lines with same _id\n",
    "#     params:\n",
    "#     returns:\n",
    "#     \"\"\"\n",
    "#     grouped_content = df.groupby('_id')['content'].apply(lambda x: ''.join(x.astype(str)))\n",
    "#     # Reset the index to turn the Series back into a DataFrame\n",
    "#     result_df = grouped_content.reset_index(name='concatenated_content')\n",
    "#     return result_df\n",
    "\n",
    "# def define_passages(df, global_passage_id, passage_length):\n",
    "#     \"\"\"\n",
    "#     Splits a document into passages of sentences.\n",
    "    \n",
    "#     Expects df on format \"_id\", \"concatenated_content\"\n",
    "    \n",
    "#     Params:\n",
    "#     - df: DataFrame with the documents concatenated by '_id'\n",
    "#     - passage_size: The number of sentences in each passage (default is 10)\n",
    "\n",
    "#     Returns:\n",
    "#     - DataFrame with the columns \"_id\", \"concatenated_content\", \"Passage_id\", \"passage\"\n",
    "#     \"\"\"\n",
    "#     # Initialize the tokenizer\n",
    "#     tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#     # Prepare a list to store the passages data\n",
    "#     passages_data = []\n",
    "\n",
    "\n",
    "#     # Iterate through each document\n",
    "#     for index, row in df.iterrows():\n",
    "#         doc_id = row['_id']\n",
    "#         content = row['concatenated_content']\n",
    "        \n",
    "#         # Tokenize the document into sentences\n",
    "#         sentences = tokenizer.tokenize(content)\n",
    "\n",
    "#         # Split sentences into passages\n",
    "#         for i in range(0, len(sentences), passage_size):\n",
    "#             passage = ' '.join(sentences[i:i+passage_size])\n",
    "#             passages_data.append([doc_id, global_passage_id, passage])\n",
    "#             global_passage_id += 1\n",
    "\n",
    "#     # Create a new DataFrame with the passages data\n",
    "#     passages_df = pd.DataFrame(passages_data, columns=['_id', 'Passage_id', 'passage'])\n",
    "\n",
    "#     return passages_df, global_passage_id\n",
    "\n",
    "\n",
    "# def  base_df(df,passage_length):\n",
    "\n",
    "#     #filter input df\n",
    "#     df_filtered =  df[(df['INFO ITEM TYPE'] == 'DISCOVERY_EVALUATION_REPORT') & (df['content_could_be_natural_language'] == True) & (df['possible_language'] == 'en')].reset_index()\n",
    "#     concat_df = concatonate_docs(df_filtered)\n",
    "#     global_passage_id = 0\n",
    "#     passage_df, global_passage_id = define_passages(concat_df,global_passage_id=global_passage_id,passage_length=passage_length)\n",
    "#     return passage_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
